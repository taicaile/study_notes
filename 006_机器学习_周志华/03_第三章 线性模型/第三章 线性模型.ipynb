{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  第三章线性模型\n",
    "----------------------------\n",
    "## 基本形式\n",
    "线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数,即\n",
    "$$f(\\boldsymbol{x})=w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{d} x_{d}+b$$\n",
    "一般用向量写成\n",
    "$$f(\\boldsymbol{x})=\\boldsymbol{w}^{\\mathbf{T}} \\boldsymbol{x}+b$$\n",
    "where,\n",
    "$$\n",
    "\\boldsymbol{w}=\n",
    "\\begin{bmatrix}\n",
    " w_{1}  \\\\\n",
    " w_{2}  \\\\\n",
    " \\vdots \\\\\n",
    " w_{d}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "linear regression 试图学得一个线性模型尽可能准确的预测输出标记.\n",
    "\n",
    "对于离散数据的数据预处理,\n",
    "- 若存在“序”的关系\n",
    "    - 二值属性,例如身高取值,高和矮,可以转化为0和1\n",
    "    - 三值属性,例如高度的,高,中,矮,可转化为{1.0,0.5,0}\n",
    "- 若属性值不存在\"序\"的关系\n",
    "    - 瓜类的取值\"西瓜\",\"南瓜\",\"黄瓜\"可以转化为(0,0,1),(0,1,0),(1,0,0)\n",
    "    \n",
    "如何确定w和b,利用最小二乘法, 使得拟合的曲线到样本点的方差和最小\n",
    "$$\n",
    "\\begin{aligned}\\left(w^{*}, b^{*}\\right) &=\\underset{(w, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(f\\left(x_{i}\\right)-y_{i}\\right)^{2} \\\\ &=\\underset{(w, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2} \\end{aligned}\n",
    "$$\n",
    "$$\\Downarrow$$\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\partial E_{(w, b)}}{\\partial w} &=2\\left(w \\sum_{i=1}^{m} x_{i}^{2}-\\sum_{i=1}^{m}\\left(y_{i}-b\\right) x_{i}\\right) \\\\ \\frac{\\partial E_{(w, b)}}{\\partial b} &=2\\left(m b-\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\\right) \\end{aligned}\n",
    "$$\n",
    "$$\\Downarrow$$\n",
    "$$\n",
    "w=\\frac{\\sum_{i=1}^{m} y_{i}\\left(x_{i}-\\overline{x}\\right)}{\\sum_{i=1}^{m} x_{i}^{2}-\\frac{1}{m}\\left(\\sum_{i=1}^{m} x_{i}\\right)^{2}}\n",
    "$$\n",
    "$$\n",
    "b=\\frac{1}{m} \\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最小二乘法向量化**\n",
    "$$\n",
    "\\mathbf{X}=\\left( \\begin{array}{ccccc}{x_{11}} & {x_{12}} & {\\dots} & {x_{1 d}} & {1} \\\\ {x_{21}} & {x_{22}} & {\\dots} & {x_{2 d}} & {1} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} & {\\vdots} \\\\ {x_{m 1}} & {x_{m 2}} & {\\dots} & {x_{m d}} & {1}\\end{array}\\right)=\\left( \\begin{array}{cc}{\\boldsymbol{x}_{1}^{\\mathrm{T}}} & {1} \\\\ {\\boldsymbol{x}_{2}^{\\mathrm{T}}} & {1} \\\\ {\\vdots} & {\\vdots} \\\\ {\\boldsymbol{x}_{m}^{\\mathrm{T}}} & {\\mathrm{1}}\\end{array}\\right)\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{y}=\n",
    "\\begin{bmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{m}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\hat{\\boldsymbol{w}}}^{*}=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min }(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})\n",
    "$$\n",
    "令, $$\n",
    "E_{\\hat{\\boldsymbol{w}}}=(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})\n",
    "$$\n",
    "$$\\Downarrow$$\n",
    "$$\n",
    "\\frac{\\partial E_{\\hat{\\boldsymbol{w}}}}{\\partial \\hat{\\boldsymbol{w}}}=2 \\mathbf{X}^{\\mathrm{T}}(\\mathbf{X} \\hat{\\boldsymbol{w}}-\\boldsymbol{y})\n",
    "$$\n",
    "当$\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}$为满秩矩阵或者正定矩阵的时候,令上式为0可得\n",
    "$$\n",
    "\\hat{\\boldsymbol{w}}^{*}=\\left(\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y}\n",
    "$$\n",
    "如果$\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}$不是满秩矩阵,可引入正则化项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑斯蒂回归\n",
    "线性模型虽然简单, 却有丰富的变化. 例如对于样例$(\\boldsymbol{x}, y), y \\in \\mathbb{R}$, 当我们希望线性模型的预测值逼近真是标记$y$时, 就得到了线性回归模型.\n",
    "$$y=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b$$\n",
    "$$\\Downarrow$$\n",
    "$$\n",
    "\\ln y=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\n",
    "$$\n",
    "上式就是\"对数线性回归\"(log-linear regression), 他实际上让 $e^{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b}$ 逼急y, 他们之间关系如下图\n",
    "![](./figs/log_linear.jpg)\n",
    "\n",
    "更一般的, 考虑单调可微函数g(), 令\n",
    "$$\n",
    "y=g^{-1}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\\right)\n",
    "$$\n",
    "这样得到的模型称为 \"广义线性模型\"(generalized linear model), 其中g()称为联系函数(link function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数几率回归\n",
    "考虑二分类任务, 其输出标记 $y \\in\\{0,1\\}$ , 而线性模型产生的预测值 $z=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b$ 为实值, 于是我们需要将实值z转换为0/1值. 最理想的时\"单位阶跃函数\"(unit-step function), 如下图所示\n",
    "$$\n",
    "y=\\left\\{\\begin{array}{cl}{0,} & {z<0} \\\\ {0.5,} & {z=0} \\\\ {1,} & {z>0}\\end{array}\\right.\n",
    "$$\n",
    "![](./figs/unit_logit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但从上图可以看出,单位阶跃函数不连续,因此不能直接作为 $g^{-}()$.\n",
    "\n",
    "于是我们希望找到一定程度上近似单位阶跃函数的\"替代函数\", 并希望它单调可微. 对数几率函数(logistic function)正是这样的一个常用替代函数:\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "其曲线如上图所示. 对数几率函数是一种\"Sigmoid函数\", 它将z值转化为一个接近0或1的y值,并且其输出值在z=0附近变化很陡.将对数几率函数作为$g^{-}()$,得到\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\\right)}}\n",
    "$$\n",
    "$w^Tx+b$对应的输出如下:\n",
    "$$\n",
    "\\ln \\frac{y}{1-y}=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将y视为样本x作为正例的可能性, 则1-y是其反例的可能性,两者的比值\n",
    "$$\n",
    "\\frac{y}{1-y}\n",
    "$$\n",
    "称为\"几率\"(odds), 反映了x作为正例的相对可能性.对几率取对数则得到\"对数几率\"(log odds,亦称为logit)\n",
    "$$\n",
    "\\ln \\frac{y}{1-y}\n",
    "$$\n",
    "由此可以看出, 上式实际上使用线性回归模型的预测结果去逼近真实标记的对数几率, 因此,其对应的模型称为\"对数几率回归\", 特别需要注意到, 虽然他的名字是回归, 但实际确实一种分类学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何确定w和b参数, 可以将y视为类别后验概率估计$p(y=1|x)$, 则上式可写为\n",
    "$$\n",
    "\\ln \\frac{p(y=1 | \\boldsymbol{x})}{p(y=0 | \\boldsymbol{x})}=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned} p(y&=1 | \\boldsymbol{x} )=\\frac{e^{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b}}{1+e^{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b}} \\\\ p(y&=0 | \\boldsymbol{x} )=\\frac{1}{1+e^{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b}} \\end{aligned}\n",
    "$$\n",
    "于是, 我们可以用极大似然法来估计w和b. 给定数据集$\\left\\{\\left(\\boldsymbol{x}_{i}, y_{i}\\right)\\right\\}_{i=1}^{m}$, 最大化最大似然\n",
    "$$\n",
    "\\ell(\\boldsymbol{w}, b)=\\sum_{i=1}^{m} \\ln p\\left(y_{i} | \\boldsymbol{x}_{i} ; \\boldsymbol{w}, b\\right)\n",
    "$$\n",
    "即令每个样本属于其真是标记的概率越大越好. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据牛顿法得到的公式如下\n",
    "$$\n",
    "\\boldsymbol{\\beta}^{t+1}=\\boldsymbol{\\beta}^{t}-\\left(\\frac{\\partial^{2} \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^{\\mathrm{T}}}\\right)^{-1} \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性判别分析(Linear Discriminant Analysis)\n",
    "![](./figs/LDA.jpg)\n",
    "- 投影后两个类别的点的距离尽可能的大\n",
    "- 同一类别的点尽可能的近\n",
    "\n",
    "投影函数$y=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}$, $w^T$可以看作\n",
    "- 向量, 两个向量的inner product 等于向量$\\vec{x}$投影到向量$w^T$上再乘以$w^T$的长度\n",
    "- 可以看作每个特性$x_i$都对应一个权重,$w^T_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{cl}{\\min _{\\boldsymbol{w}}} & {-\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b} \\boldsymbol{w}} \\\\ {\\text { s.t. }} & {\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{w} \\boldsymbol{w}=1}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类学习\n",
    "经典拆分策略\n",
    "- 一对一\n",
    "- 一对其余\n",
    "- 多对多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一对一\n",
    "将N个类别两两配对, 从而产生 $N(N-1)/2$个二分类任务, 于是我们得到$N(N-1)/2$个分类结果, 最终结果通过投票产生."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一对其余\n",
    "每次将一个类的样例作为正例,所有其他类的样例作为反例来训练N个分类器. \n",
    "- 在测试时若有一个分类器预测为正类, 则对应的类别标记作为最终分类结果。 \n",
    "- 若有多个分类器预测为正类,则通常考虑各分类器的预测置信度, 选择置信度最大的类别标记作为分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多对多\n",
    "类别划分通过\"编码矩阵\"(coding matrix)指定,编码矩阵有多种形式, 常见的主要有二元码和三元码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别不平衡问题\n",
    "类别不平衡就是指分类任务中不同类别的训练样例数目差别很大的情况。\n",
    "$$\n",
    "\\frac{y^{\\prime}}{1-y^{\\prime}}=\\frac{y}{1-y} \\times \\frac{m^{-}}{m^{+}}\n",
    "$$\n",
    "- $m^{+}$表示正例数目\n",
    "- $m^{-}$表示负例数目\n",
    "这就是类别不平衡学习的一个基本策略 --- \"在缩放\"(rescaling).\n",
    "- 直接对训练集里的反类样例进行\"欠采样\",即去除一些反例使得正反例数目接近,然后再进行学习;\n",
    "- 第二类是对训练集里的正类样例进行\"过采样\",即增加一些正例使得正反例 数目接近,然后再进行学习\n",
    "- 第三类则是直接基于原始训练集进行学习,但在用训练好的分类器进行预测时,将上式嵌入到决策过程中,称为\"阈值便宜\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
