{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯分类器\n",
    "## 贝叶斯决策论\n",
    "基于后验概率$P\\left(c_{i} | x\\right)$可获得将样本x分类为$c_i$所产生的期望损失, 即在样本x上的条件风险\n",
    "$$\n",
    "\\begin{equation}\n",
    "R\\left(c_{i} | \\boldsymbol{x}\\right)=\\sum_{j=1}^{N} \\lambda_{i j} P\\left(c_{j} | \\boldsymbol{x}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\\lambda_{i j}=\\left\\{\\begin{array}{ll}{0,} & {\\text { if } i=j} \\\\ {1,} & {\\text { otherwise }}\\end{array}\\right.$$\n",
    "\n",
    "- $\\mathcal{Y}=\\left\\{c_{1}, c_{2}, \\ldots, c_{N}\\right\\}$, 即有N种可能的类别标记.\n",
    "- $\\lambda_{i j}$ 是将一个真实标记$c_j$的样本误分类额外$c_i$所产生的损失.\n",
    "\n",
    "我们的任务是寻找一个判定准则 $h : \\mathcal{X} \\mapsto \\mathcal{Y}$ 以最小化总体风险\n",
    "$$\n",
    "R(h)=\\mathbb{E}_{\\boldsymbol{x}}[R(h(\\boldsymbol{x}) | \\boldsymbol{x})]\n",
    "$$\n",
    "\n",
    "显然, 对每个样本x,若h能最小化条件风险$R(h(\\boldsymbol{x}) | \\boldsymbol{x})$, 则总体风险$R(h)$也将被最小化. 这就产生了贝叶斯判定准则: 为最小化总体风险, 只需在每个样本上选择那个能使条件风险$R(c | x)$最小的类别标记, 即\n",
    "$$\n",
    "h^{*}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\min } R(c | \\boldsymbol{x})\n",
    "$$\n",
    "此时, $h^*$称为贝叶斯最优分类器, 与之对应的总体风险$R(h^*)$称为贝叶斯风险.\n",
    "$1-R(h^*)$反映了分类器所能达到的最好性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难看出, 欲使用贝叶斯判定准则来最小化决策风险, 首先要获得后验概率 $P(c | x)$.然而, 在现实任务中这通常难以获得. 从这个角度来看, 机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出 后验概率$P(c | x)$.\n",
    "\n",
    "大体来说, 主要有两种策略\n",
    "- 给定 x, 可通过直接建模 $P(c | \\boldsymbol{x})$ 来预测 $c$, 这样得到的是 判别式模型\n",
    "- 也可先对联合概率分布 $P(x,c)$建模, 然后再由此获得 P(c|x), 这样得到的是生成式模型.\n",
    "\n",
    "前面介绍的决策树, BP神经网络, 支持向量机等, 都可归入判别式模型的范畴。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 极大似然估计\n",
    "概率模型的训练过程就是参数估计过程. 对于参数估计, 统计学界的两个学派分别提供了不同的解决方案: \n",
    "- 频率主义学派认为参数虽然位置, 但却是客观存在的固定值, 因此, 可通过优化似然函数等准则来确定参数值; \n",
    "- 贝叶斯学派则认为参数是未观察到的随机变量, 其本身也可有分布, 因此, 可假定参数服从一个先验分布, 然后基于观测到的数据来计算参数的后验分布. \n",
    "\n",
    "极大似然估计就是来自于频率主义学派, 根据数据采样来估计频率分布参数的经典方法.\n",
    "\n",
    "直观上看, 极大似然估计是试图再 $\\theta_c$所有可能的取值中, 找到一个能使数据出现的可能性最大的值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类器\n",
    "基于贝叶斯公式来估计后验概率$P(c|x)$的主要困难在于: 类条件概率 $P(x|c)$是所有属性上的联合概率, 难以从有限的训练样本直接估计而得. 为避开这个障碍, 朴素贝叶斯分类器采用了\"属性条件独立性假设\": 对已知类别, 假设所有属性相互独立, 换言之, 假设每个属性独立地对分类结果发生影响.\n",
    "\n",
    "基于属性条件独立性假设, 贝叶斯公式可重写为\n",
    "\n",
    "$$\n",
    "P(c | \\boldsymbol{x})=\\frac{P(c) P(\\boldsymbol{x} | c)}{P(\\boldsymbol{x})}=\\frac{P(c)}{P(\\boldsymbol{x})} \\prod_{i=1}^{d} P\\left(x_{i} | c\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 半朴素贝叶斯分类器\n",
    "半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息, 从而即不需进行完全联合概率计算, 又不至于彻底忽略了比较强的属性依赖关系.\"独依赖估计\"是半朴素贝叶斯分类器最常用的一种策略. 所谓\"独依赖\"就是假设每个属性再类别之外最多仅依赖于一个其他属性, 即\n",
    "$$\n",
    "P(c | \\boldsymbol{x}) \\propto P(c) \\prod_{i=1}^{d} P\\left(x_{i} | c, p a_{i}\\right)\n",
    "$$\n",
    "- 这里$P\\left(x_{i} | c, p a_{i}\\right)$可以理解为在已知$(c, pa_i)$发生的前提下, $p(x_i)$的概率.\n",
    "\n",
    "\n",
    "这里 $pa_i$为属性$x_i$所依赖的属性, 称为 $x_i$ 的父属性.\n",
    "\n",
    "于是, 问题的关键转化为如何确定每个属性的父属性, 不同的做法产生不同的独依赖分类器器.\n",
    "\n",
    "### SPODE(Super-Parent ODE) 方法\n",
    "\n",
    "### TAN(Tree Augmented naive Bayes)\n",
    "\n",
    "### AODE(Averaged One-Dependent Estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯网\n",
    "贝叶斯网借助有向无环图来刻画属性之间的依赖关系, 并使用条件概率表来描述属性的联合概率分布.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM 算法\n",
    "在现实应用中往往会遇到\"不完整\"的训练样本, 例如由于西瓜的根蒂已脱落, 无法看出是\"蜷缩\"还是\"硬挺\", 则训练样本的\"根蒂\"属性变量值未知. 这种存在\"未观测\"变量称为隐变量.\n",
    "$$\n",
    "L L(\\Theta | \\mathbf{X}, \\mathbf{Z})=\\ln P(\\mathbf{X}, \\mathbf{Z} | \\Theta)\n",
    "$$\n",
    "- $\\mathbf{X}$ 表示已观测变量集\n",
    "- $\\mathbf{Z}$ 是隐变量集\n",
    "- $\\Theta$ 表示模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是, 以初始值 $\\theta_0$为起点, 可迭代执行以下步骤直至收敛:\n",
    "- 基于$\\theta_0$推断隐变量Z的期望, 记为 $Z^t$\n",
    "- 基于已观测变量 $X$ 和 $Z^t$ 对参数 $\\theta$ 做极大似然估计, 记为 $\\theta^{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
