{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经元模型\n",
    "神经元接收到来自n个其他神经元传递过来的输入信号, 这些输入信号通过带权重的连接进行传递, 神经元接收到的总输入值将与神经元的阈值进行比较, 然后通过激活函数(activation function)处理以产生神经元的输出.\n",
    "![M-P神经元](./figs/M-P.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理想中的激活函数是如下图所示的阶跃函数,它将输入值映射为输出值\"0\"或\"1\", 显然\"1\"对应于神经元兴奋,\"0\"对应于神经元抑制. 然而,阶跃函数具有不连续, 不光滑等不太好的性质, 因此实际常用Sigmoid函数作为激活函数, 典型的Sigmoid函数如图所示,他把较大范围内变化的输入值挤压到(0,1)输出值范围内，下图为典型的神经元激活函数\n",
    "![step and sigmoid function](./figs/step-sigmoid.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把许多个这样的神经元按一定的层次结构连接起来,就得到了神经网络."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机与多层网络\n",
    "感知机(Perceptron)由两层神经元组成, 输入层接收外界信号后传递给输出层, 输出层是M-P神经元.\n",
    "\n",
    "注意到 $y=f\\left(\\sum_{i}^{n} w_{i} x_{i}-\\theta\\right)$. 阈值 $-\\theta$ 可看作一个固定输入为 $-1.0$ 的哑节点(dummy node)所对应的连接权重$w_{n+1}$, 这样, 权重和阈值的学习就可以统一为权重的学习. 即 y可以表示为$f\\left(\\sum_{i}^{n+1} \\omega_{i} x_{i}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机的学习规则, 对训练样例$(\\boldsymbol{x}, y)$, 若当前感知机的输出为$\\hat{y}$.\n",
    "这里定义感知机的cost function为分类错误的点到超平面的距离\n",
    "\n",
    "$$d=\\frac{|\\vec{w} \\cdot \\vec{x}|}{||w||}=(y-\\hat{y})\\frac{1}{\\|\\omega\\|} \\vec{\\omega} \\cdot \\vec{x}$$\n",
    "\n",
    "- 当 $y$ 和 $\\hat{y}$ 一致时, 距离为0\n",
    "- 当 $y>0$, $\\vec{\\omega} \\cdot \\vec{x}<0$时, $\\Rightarrow (y-\\hat{y})>0, d<0$\n",
    "- 当 $y<0$, $\\vec{\\omega} \\cdot \\vec{x}>0$时, $\\Rightarrow (y-\\hat{y})<0, d<0$\n",
    "\n",
    "注意这里的距离是负的, 也就是说是错误的分类距离。 我们的目标是使错误分类点到平面的距离和最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, CostFunction: $$L(\\omega)=\\sum_{i \\in M}(y-\\hat{y}) w_{i} x_{i}$$\n",
    "- 其中M时所有分类错误点的集合.\n",
    "$$\\Downarrow$$\n",
    "$$\\text{minimize} L(\\omega)$$\n",
    "\n",
    "这里利用随机梯度下降法优化目标函数,使其达到最小值。 \n",
    "$$\n",
    "\\frac{d L(\\omega)}{d \\omega}=\\sum_{i \\in M}(y-\\hat{y}) x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}{w_{i} \\leftarrow w_{i}+\\Delta w_{i}} \\\\ {\\Delta w_{i}=\\eta(y-\\hat{y}) x_{i}}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机工作的前提是问题都是线性可分的的, 即存在一个平面能将他们分开。\n",
    "\n",
    "要解决非线性可分问题, 需要考虑使用多层功能神经元. 例如下图中这个简单的两层感知机能够解决\n",
    "![](./figs/xor.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差逆传播算法\n",
    "误逆差传播算法(error BackPropagation, 简称BP)算法是迄今最成功的神经网络学习算法.\n",
    "BP网络及算法中的变量符号, 假设隐层和输出层神经元都是用sigmoid函数\n",
    "![](./figs/bp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练例$\\left(\\boldsymbol{x}_{k}, \\boldsymbol{y}_{k}\\right)$, 假定神经网络的输出为 $\\hat{\\boldsymbol{y}}_{k}=\\left(\\hat{y}_{1}^{k}, \\hat{y}_{2}^{k}, \\ldots, \\hat{y}_{l}^{k}\\right)$, 即 \n",
    "$$\n",
    "\\hat{y}_{j}^{k}=f\\left(\\beta_{j}-\\theta_{j}\\right)\n",
    "$$\n",
    "则网络在$(x_k, y_k)$的均方误差为\n",
    "$$\n",
    "E_{k}=\\frac{1}{2} \\sum_{j=1}^{l}\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right)^{2}\n",
    "$$\n",
    "上图BP网络中有$(d+l+1)q+l$ 个参数需确定: \n",
    "- 输入层到隐层的 $d\\times q$个权值, \n",
    "- 隐层到输出层的 $q\\times l$个权值、\n",
    "- $q$个隐层神经元的阈值、\n",
    "- $l$个输出层神经元的阈值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP是一种迭代学习算法, 在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计, 任意参数v的更新估计式为\n",
    "$$\n",
    "v \\leftarrow v+\\Delta v\n",
    "$$\n",
    "下面以上图bp神经网络的隐层到输出层的连接权$w_{hj}$为例来进行推导.\n",
    "\n",
    "BP算法基于梯度下降策略, 以目标的负梯度方向对参数进行调整. \n",
    "$$\n",
    "\\Delta w_{h j}=-\\eta \\frac{\\partial E_{k}}{\\partial w_{h j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E_{k}}{\\partial w_{h j}}=\\frac{\\partial E_{k}}{\\partial \\hat{y}_{j}^{k}} \\cdot \\frac{\\partial \\hat{y}_{j}^{k}}{\\partial \\beta_{j}} \\cdot \\frac{\\partial \\beta_{j}}{\\partial w_{h j}}\n",
    "$$\n",
    "根据$\\beta_{j}$的定义, 显然有\n",
    "$$\n",
    "\\frac{\\partial \\beta_{j}}{\\partial w_{h j}}=b_{h}\n",
    "$$\n",
    "Sigmoid函数有一个很好的性质:\n",
    "$$\n",
    "f^{\\prime}(x)=f(x)(1-f(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned} g_{j} &=-\\frac{\\partial E_{k}}{\\partial \\hat{y}_{j}^{k}} \\cdot \\frac{\\partial \\hat{y}_{j}^{k}}{\\partial \\beta_{j}} \\\\ &=-\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right) f^{\\prime}\\left(\\beta_{j}-\\theta_{j}\\right) \\\\ &=\\hat{y}_{j}^{k}\\left(1-\\hat{y}_{j}^{k}\\right)\\left(y_{j}^{k}-\\hat{y}_{j}^{k}\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta w_{h j}=\\eta g_{j} b_{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
