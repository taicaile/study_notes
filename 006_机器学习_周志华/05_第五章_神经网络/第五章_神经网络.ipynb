{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经元模型\n",
    "神经元接收到来自n个其他神经元传递过来的输入信号, 这些输入信号通过带权重的连接进行传递, 神经元接收到的总输入值将与神经元的阈值进行比较, 然后通过激活函数(activation function)处理以产生神经元的输出.\n",
    "![M-P神经元](./figs/M-P.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理想中的激活函数是如下图所示的阶跃函数,它将输入值映射为输出值\"0\"或\"1\", 显然\"1\"对应于神经元兴奋,\"0\"对应于神经元抑制. 然而,阶跃函数具有不连续, 不光滑等不太好的性质, 因此实际常用Sigmoid函数作为激活函数, 典型的Sigmoid函数如图所示,他把较大范围内变化的输入值挤压到(0,1)输出值范围内，下图为典型的神经元激活函数\n",
    "![step and sigmoid function](./figs/step-sigmoid.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把许多个这样的神经元按一定的层次结构连接起来,就得到了神经网络."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机与多层网络\n",
    "感知机(Perceptron)由两层神经元组成, 输入层接收外界信号后传递给输出层, 输出层是M-P神经元.\n",
    "\n",
    "注意到 $y=f\\left(\\sum_{i}^{n} w_{i} x_{i}-\\theta\\right)$. 阈值 $-\\theta$ 可看作一个固定输入为 $-1.0$ 的哑节点(dummy node)所对应的连接权重$w_{n+1}$, 这样, 权重和阈值的学习就可以统一为权重的学习. 即 y可以表示为$f\\left(\\sum_{i}^{n+1} \\omega_{i} x_{i}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机的学习规则, 对训练样例$(\\boldsymbol{x}, y)$, 若当前感知机的输出为$\\hat{y}$.\n",
    "这里定义感知机的cost function为分类错误的点到超平面的距离\n",
    "\n",
    "$$d=\\frac{|\\vec{w} \\cdot \\vec{x}|}{||w||}=(y-\\hat{y})\\frac{1}{\\|\\omega\\|} \\vec{\\omega} \\cdot \\vec{x}$$\n",
    "\n",
    "- 当 $y$ 和 $\\hat{y}$ 一致时, 距离为0\n",
    "- 当 $y>0$, $\\vec{\\omega} \\cdot \\vec{x}<0$时, $\\Rightarrow (y-\\hat{y})>0, d<0$\n",
    "- 当 $y<0$, $\\vec{\\omega} \\cdot \\vec{x}>0$时, $\\Rightarrow (y-\\hat{y})<0, d<0$\n",
    "\n",
    "注意这里的距离是负的, 也就是说是错误的分类距离。 我们的目标是使错误分类点到平面的距离和最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, CostFunction: $$L(\\omega)=\\sum_{i \\in M}(y-\\hat{y}) w_{i} x_{i}$$\n",
    "- 其中M时所有分类错误点的集合.\n",
    "$$\\Downarrow$$\n",
    "$$\\text{minimize} L(\\omega)$$\n",
    "\n",
    "这里利用随机梯度下降法优化目标函数,使其达到最小值。 \n",
    "$$\n",
    "\\frac{d L(\\omega)}{d \\omega}=\\sum_{i \\in M}(y-\\hat{y}) x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}{w_{i} \\leftarrow w_{i}+\\Delta w_{i}} \\\\ {\\Delta w_{i}=\\eta(y-\\hat{y}) x_{i}}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机工作的前提是问题都是线性可分的的, 即存在一个平面能将他们分开。\n",
    "\n",
    "要解决非线性可分问题, 需要考虑使用多层功能神经元. 例如下图中这个简单的两层感知机能够解决\n",
    "![](./figs/xor.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差逆传播算法\n",
    "误逆差传播算法(error BackPropagation, 简称BP)算法是迄今最成功的神经网络学习算法.\n",
    "BP网络及算法中的变量符号, 假设隐层和输出层神经元都是用sigmoid函数\n",
    "![](./figs/bp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练例$\\left(\\boldsymbol{x}_{k}, \\boldsymbol{y}_{k}\\right)$, 假定神经网络的输出为 $\\hat{\\boldsymbol{y}}_{k}=\\left(\\hat{y}_{1}^{k}, \\hat{y}_{2}^{k}, \\ldots, \\hat{y}_{l}^{k}\\right)$, 即 \n",
    "$$\n",
    "\\hat{y}_{j}^{k}=f\\left(\\beta_{j}-\\theta_{j}\\right)\n",
    "$$\n",
    "则网络在$(x_k, y_k)$的均方误差为\n",
    "$$\n",
    "E_{k}=\\frac{1}{2} \\sum_{j=1}^{l}\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right)^{2}\n",
    "$$\n",
    "上图BP网络中有$(d+l+1)q+l$ 个参数需确定: \n",
    "- 输入层到隐层的 $d\\times q$个权值, \n",
    "- 隐层到输出层的 $q\\times l$个权值、\n",
    "- $q$个隐层神经元的阈值、\n",
    "- $l$个输出层神经元的阈值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP是一种迭代学习算法, 在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计, 任意参数v的更新估计式为\n",
    "$$\n",
    "v \\leftarrow v+\\Delta v\n",
    "$$\n",
    "下面以上图bp神经网络的隐层到输出层的连接权$w_{hj}$为例来进行推导.\n",
    "\n",
    "BP算法基于梯度下降策略, 以目标的负梯度方向对参数进行调整. \n",
    "$$\n",
    "\\Delta w_{h j}=-\\eta \\frac{\\partial E_{k}}{\\partial w_{h j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E_{k}}{\\partial w_{h j}}=\\frac{\\partial E_{k}}{\\partial \\hat{y}_{j}^{k}} \\cdot \\frac{\\partial \\hat{y}_{j}^{k}}{\\partial \\beta_{j}} \\cdot \\frac{\\partial \\beta_{j}}{\\partial w_{h j}}\n",
    "$$\n",
    "根据$\\beta_{j}$的定义, 显然有\n",
    "$$\n",
    "\\frac{\\partial \\beta_{j}}{\\partial w_{h j}}=b_{h}\n",
    "$$\n",
    "Sigmoid函数有一个很好的性质:\n",
    "$$\n",
    "f^{\\prime}(x)=f(x)(1-f(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned} g_{j} &=-\\frac{\\partial E_{k}}{\\partial \\hat{y}_{j}^{k}} \\cdot \\frac{\\partial \\hat{y}_{j}^{k}}{\\partial \\beta_{j}} \\\\ &=-\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right) f^{\\prime}\\left(\\beta_{j}-\\theta_{j}\\right) \\\\ &=\\hat{y}_{j}^{k}\\left(1-\\hat{y}_{j}^{k}\\right)\\left(y_{j}^{k}-\\hat{y}_{j}^{k}\\right) \\end{aligned}\n",
    "$$\n",
    "$$\\Downarrow$$\n",
    "$$\n",
    "\\Delta w_{h j}=\\eta g_{j} b_{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似可得\n",
    "$$\n",
    "\\Delta \\theta_{j}=-\\eta g_{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于:\n",
    "$$\n",
    "\\Delta \\theta_{j}=-\\eta \\frac{\\partial E_{k}}{\\partial \\theta_{j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\partial E_{k}}{\\partial \\theta_{j}} &=\\frac{\\partial E_{k}}{\\partial \\hat{y}_{j}^{k}} \\cdot \\frac{\\partial \\hat{y}_{j}^{k}}{\\partial \\theta_{j}} \\\\ &=\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right) \\cdot f^{\\prime}\\left(\\beta_{j}-\\theta_{j}\\right) \\cdot(-1) \\\\ &=-\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right) f^{\\prime}\\left(\\beta_{j}-\\theta_{j}\\right) \\\\ &=g_{j} \\end{aligned}\n",
    "$$\n",
    "所以:\n",
    "$$\n",
    "\\Delta \\theta_{j}=-\\eta \\frac{\\partial E_{k}}{\\partial \\theta_{j}}=-\\eta g_{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似可得:\n",
    "$$\\Delta v_{ih} = \\eta e_h x_i$$ \n",
    "因为 \n",
    "$$\\Delta v_{ih} = -\\eta \\cfrac{\\partial E_k}{\\partial v_{ih}}$$ \n",
    "又 \n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\cfrac{\\partial E_k}{\\partial v_{ih}} \n",
    "&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}j^k} \\cdot \\cfrac{\\partial \\hat{y}j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot \\cfrac{\\partial b_h}{\\partial \\alpha_h} \\cdot \\cfrac{\\partial \\alpha_h}{\\partial v{ih}} \\\\ \n",
    "&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}j^k} \\cdot \\cfrac{\\partial \\hat{y}j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot \\cfrac{\\partial b_h}{\\partial \\alpha_h} \\cdot x_i \\\\ \n",
    "&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}j^k} \\cdot \\cfrac{\\partial \\hat{y}j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot f’(\\alpha_h-\\gamma_h) \\cdot x_i \\\\\n",
    "&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}j^k} \\cdot \\cfrac{\\partial \\hat{y}j^k}{\\partial \\beta_j} \\cdot w{hj} \\cdot f’(\\alpha_h-\\gamma_h) \\cdot x_i \\\\ \n",
    "&= \\sum_{j=1}^{l} (-g_j) \\cdot w{hj} \\cdot f’(\\alpha_h-\\gamma_h) \\cdot x_i \\\\ \n",
    "&= -f’(\\alpha_h-\\gamma_h) \\cdot \\sum_{j=1}^{l} g_j \\cdot w_{hj} \\cdot x_i \\\\  \n",
    "&= -b_h(1-b_h) \\cdot \\sum_{j=1}^{l} g_j \\cdot w_{hj} \\cdot x_i \\\\ \n",
    "&= -e_h \\cdot x_i \\end{aligned} $$ \n",
    "\n",
    "所以 \n",
    "$$\\Delta v_{ih} = -\\eta \\cdot -e_h \\cdot x_i=\\eta e_h x_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理可得\n",
    "$$\\Delta \\gamma_h= -\\eta e_h$$ \n",
    "因为 \n",
    "$$\\Delta \\gamma_h = -\\eta \\cfrac{\\partial E_k}{\\partial \\gamma_h}$$ \n",
    "又 \n",
    "$$ \\begin{aligned} \\cfrac{\\partial E_k}{\\partial \\gamma_h} &= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot \\cfrac{\\partial b_h}{\\partial \\gamma_h} \\\\ \n",
    "&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot f’(\\alpha_h-\\gamma_h) \\cdot (-1) \\\\ \n",
    "&= -\\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}j^k}{\\partial \\beta_j} \\cdot w{hj} \\cdot f’(\\alpha_h-\\gamma_h)\\\\ \n",
    "&=e_h \\end{aligned} $$ \n",
    "所以 \n",
    "$$\\Delta \\gamma_h= -\\eta e_h$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面式子中\n",
    "$$\n",
    "\\begin{aligned} e_{h} &=-\\frac{\\partial E_{k}}{\\partial b_{h}} \\cdot \\frac{\\partial b_{h}}{\\partial \\alpha_{h}} \\\\ \n",
    "&=-\\sum_{j=1}^{l} \\frac{\\partial E_{k}}{\\partial \\beta_{j}} \\cdot \\frac{\\partial \\beta_{j}}{\\partial b_{h}} f^{\\prime}\\left(\\alpha_{h}-\\gamma_{h}\\right) \\\\\n",
    "&=\\sum_{j=1}^{l} w_{h j} g_{j} f^{\\prime}\\left(\\alpha_{h}-\\gamma_{h}\\right) \\\\ \n",
    "&=b_{h}\\left(1-b_{h}\\right) \\sum_{j=1}^{l} w_{h j} g_{j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\\begin{array}{l}{}\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 标准BP算法每次更新只针对单个样例, 因此更新比较频繁\n",
    "- 累积BP算法直接针对累计误差最小化, 它在读取整个训练集D一遍后才针对参数进行更新,其更新参数的频率低的多.\n",
    "\n",
    "解决过拟合方法\n",
    "- 一种策略是早停, 若训练集误差降低但验证集误差升高, 则停止训练, 同时返回具有最小验证集误差的连接权和阈值.\n",
    "- 第二种策略是\"正则化\", 其基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分, 例如连接权和阈值的平方和. 仍令$E_k$表示第k个训练样例上的误差, $w_i$表示连接权和阈值, 则误差目标函数改变为\n",
    "$$\n",
    "E=\\lambda \\frac{1}{m} \\sum_{k=1}^{m} E_{k}+(1-\\lambda) \\sum_{i} w_{i}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局最小与局部极小\n",
    "梯度下降法无法保证能获得全局最优解.\n",
    "如果存在多个局部最优解的话,沿着负梯度的方向只能保证是搜索到了局部最小值.\n",
    "- 以多组不同参数值初始化多个神经网络, 按标准方法训练后, 取其中误差最小的解作为最终参考。\n",
    "- 使用\"模拟退火\"技术. 模拟退火在每一步都以一定的概率接受比当前解更差的结果, 从而有助于\"跳出\"局部最小.\n",
    "- 使用随机梯度下降, 随机梯度下降法在计算梯度时加入了随机因素. 于是, 即便陷入局部极小点, 它计算出的梯度仍可能不为零, 这样就有机会跳出局部最小继续搜索。\n",
    "- 遗传算法也常用来训练神经网络以更好的逼近全局最优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他常见神经网络\n",
    "### RBF网络\n",
    "### ART网络\n",
    "### SOM网络\n",
    "### 级联相关网络\n",
    "### Elman 网络\n",
    "### Boltzmann 机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习\n",
    "典型的深度学习模型就是很深层的神经网络. \n",
    "\n",
    "多从增加模型复杂度的角度来看, 增加隐层的数目下显然比增加神经元的数目更有效。 然而, 多隐层神经网络难以直接用经典算法(例如标准BP算法)进行训练, 因为误差在多隐层内逆传播时, 往往会发散而不能收敛到稳定状态。\n",
    "\n",
    "无监督逐层训练(unsupervised layer-wise training)是多隐层网络训练的有效手段。\n",
    "\n",
    "另一种节省训练开销的策略是\"权共享\"(weight sharing), 即让一组神经元使用相同的连接权。 这个策略在卷积神经网络中发挥了重要作用。\n",
    "\n",
    "以CNN进行手写数字识别任务为例,网络输入是一个32x32的手写数字图像, 输出是其识别结果, CNN复合多个\"卷积层\"和\"采样层\"对输入信号进行加工, 然后在连接层实现与输出目标的映射.\n",
    "![CNN用于手写数字识别](./figs/cnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN可用BP算法进行训练, 但在训练中, 无论是卷积层还是采样层, 其每一组神经元都是用相同的连接权, 从而大幅减少了需要训练的参数数目."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
