{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c|x)=\\frac{p(x|c)p(c)}{p(x)}$$\n",
    "\n",
    "$p(c|x)$ means the probability of the c comes from x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯  \n",
    "如果有两个特征，每个特征需要10个数据，则共需要10\\*10组数据。  \n",
    "为什么需要10\\*10呢，因为特征之间不独立。不同的特征之间存在关联。\n",
    "\n",
    "特征独立是什么意思？  \n",
    "表示特征和特征之间无直接的联系，特征之间的先后发生顺序对结果影响不大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying with conditional probabilities\n",
    "\n",
    "$$p(c_i|x,y)=\\frac{p(x,y|c_i)p(c_i)}{p(x,y)}$$  \n",
    "\n",
    "if $p(c_1|x,y)>p(c_2|x,y)$, the class is $c_1$  \n",
    "if $p(c_1|x,y)<p(c_2|x,y)$, the class is $c_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document classification with naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying text with Python\n",
    "### Prepare: making word vectors from text\n",
    "- Word list to vector function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:23:59.950973Z",
     "start_time": "2018-12-23T14:23:59.924971Z"
    }
   },
   "outputs": [],
   "source": [
    "# 实验样本数据，共5个句子，每个句子包含不同长度的单词，返回数据集和标签list\n",
    "def loadDataSet():\n",
    "    postingList = [\n",
    "        ['my', 'dog', 'has', 'fela', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "    ]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  #1 is abusive, 0 not\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:23:59.976974Z",
     "start_time": "2018-12-23T14:23:59.959973Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提取出样本数据中的关键词，建立包含所有关键词的list，通过set转化为list\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)  # return unique words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:23:59.993975Z",
     "start_time": "2018-12-23T14:23:59.979974Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将inputSet作为输入单词的集合，如果单词在单词列表中，则把单词列表对应的数值置为1.\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:24:00.012976Z",
     "start_time": "2018-12-23T14:24:00.000976Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problems', 'garbage', 'food', 'has', 'not', 'buying', 'worthless', 'posting', 'him', 'love', 'stop', 'steak', 'licks', 'quit', 'dalmation', 'mr', 'is', 'I', 'stupid', 'please', 'help', 'ate', 'how', 'dog', 'maybe', 'fela', 'park', 'my', 'take', 'cute', 'to', 'so']\n",
      "[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 测试，先加载数据，在创建单词列表，\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "Vec0 = setOfWords2Vec(myVocabList, listOPosts[0])\n",
    "Vec3 = setOfWords2Vec(myVocabList, listOPosts[3])\n",
    "print(myVocabList)\n",
    "print(Vec0)\n",
    "print(Vec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:24:00.030977Z",
     "start_time": "2018-12-23T14:24:00.017977Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# trainMatrix 为文档矩阵，和每篇文档所对应的标签\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    # 获得文档矩阵的长度\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 得到输入矩阵中列的长度\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # caculate the probability of abusive sentence\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    '''\n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    '''\n",
    "    # to lessen the impact of 0 in mutiplication,the code above need to be changed as below\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # the probability for each word which comes from p1 or p0\n",
    "    '''\n",
    "    ln(a*b)  =  ln(a)+ln(b)\n",
    "    p1Vect = p1Num / p1Denom\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    '''\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:24:00.051979Z",
     "start_time": "2018-12-23T14:24:00.034978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0V: [-2.56494936 -3.25809654 -3.25809654 -2.56494936 -3.25809654 -3.25809654\n",
      " -3.25809654 -3.25809654 -2.15948425 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      " -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      " -3.25809654 -2.56494936 -3.25809654 -1.87180218 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936]\n",
      "p1V: [-3.04452244 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -2.35137526\n",
      " -1.94591015 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -3.04452244\n",
      " -1.65822808 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -1.94591015\n",
      " -2.35137526 -3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244\n",
      " -2.35137526 -3.04452244]\n",
      "pAb: 0.5\n"
     ]
    }
   ],
   "source": [
    "#加载测试数据，返回6个sentences以及6句话对应的标签。\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "#根据以上生成的6个sentences中的单词，创建一个不包含重复吃的集合。\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "#矩阵，将每句话中的单词映射到集合中。1表示存在。\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "print('p0V:', p0V)\n",
    "print('p1V:', p1V)\n",
    "print('pAb:', pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: modifying the classifier for real-world conditions \n",
    "- Naïve Bayes classify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:24:00.065979Z",
     "start_time": "2018-12-23T14:24:00.054979Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:24:00.084980Z",
     "start_time": "2018-12-23T14:24:00.071980Z"
    }
   },
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPost, listClasses = loadDataSet()\n",
    "    myVocaList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(np.array(trainMat),np.array(listClasses))\n",
    "    testEntry = ['love','my','dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as : ', classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid','garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:24:00.098981Z",
     "start_time": "2018-12-23T14:24:00.088981Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as :  0\n",
      "['stupid', 'garbage'] classified as :  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare: the bag-of-words document model\n",
    "- Naïve Bayes bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:24:00.115982Z",
     "start_time": "2018-12-23T14:24:00.101981Z"
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: classifying spam email with naïve Bayes\n",
    "- Prepare: tokenizing text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:24:10.447573Z",
     "start_time": "2018-12-23T14:24:10.427572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "mySent='This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "print(mySent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:26:17.778856Z",
     "start_time": "2018-12-23T14:26:17.755855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "'''\n",
    "r = re.compile('\\\\W*')\n",
    "listOfTokens = r.split(mySent)\n",
    "'''\n",
    "r = re.compile('\\\\w+')\n",
    "listOfTokens = r.findall(mySent)\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:28:59.556109Z",
     "start_time": "2018-12-23T14:28:59.538108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "allWords = [tok for tok in listOfTokens if len(tok) > 0]\n",
    "print(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:37:54.524708Z",
     "start_time": "2018-12-23T14:37:54.502706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'Google', 'Groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'February', '2011', 'We', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'Google', 'Groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'Instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'Google', 'Docs', 'and', 'Google', 'Sites', 'For', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'Google', 'Sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'You', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'If', 'you抮e', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'Google', 'Docs', 'You', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'Google', 'Groups', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: split() requires a non-empty pattern match.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "emailText = open('email/ham/6.txt').read()\n",
    "r = re.compile('\\\\W*')\n",
    "listOfTokens=r.split(emailText)\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
