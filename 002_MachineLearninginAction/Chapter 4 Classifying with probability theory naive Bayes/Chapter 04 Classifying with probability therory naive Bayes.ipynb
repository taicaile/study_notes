{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c|x)=\\frac{p(x|c)p(c)}{p(x)}$$\n",
    "\n",
    "$p(c|x)$ means the probability of the c comes from x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯  \n",
    "如果有两个特征，每个特征需要10个数据，则共需要10\\*10组数据。  \n",
    "为什么需要10\\*10呢，因为特征之间不独立。不同的特征之间存在关联。\n",
    "\n",
    "特征独立是什么意思？  \n",
    "表示特征和特征之间无直接的联系，特征之间的先后发生顺序对结果影响不大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying with conditional probabilities\n",
    "\n",
    "$$p(c_i|x,y)=\\frac{p(x,y|c_i)p(c_i)}{p(x,y)}$$  \n",
    "\n",
    "if $p(c_1|x,y)>p(c_2|x,y)$, the class is $c_1$  \n",
    "if $p(c_1|x,y)<p(c_2|x,y)$, the class is $c_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document classification with naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying text with Python\n",
    "### Prepare: making word vectors from text\n",
    "- Word list to vector function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.577580Z",
     "start_time": "2018-12-24T14:14:16.562579Z"
    }
   },
   "outputs": [],
   "source": [
    "# 实验样本数据，共5个句子，每个句子包含不同长度的单词，返回数据集和标签list\n",
    "def loadDataSet():\n",
    "    postingList = [\n",
    "        ['my', 'dog', 'has', 'fela', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "    ]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  #1 is abusive, 0 not\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.607582Z",
     "start_time": "2018-12-24T14:14:16.581580Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提取出样本数据中的关键词，建立包含所有关键词的list，通过set转化为list\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)  # return unique words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.630583Z",
     "start_time": "2018-12-24T14:14:16.611582Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将inputSet作为输入单词的集合，如果单词在单词列表中，则把单词列表对应的数值置为1.\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.647584Z",
     "start_time": "2018-12-24T14:14:16.634583Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'licks', 'not', 'please', 'help', 'him', 'fela', 'take', 'is', 'park', 'steak', 'worthless', 'to', 'maybe', 'dalmation', 'posting', 'has', 'ate', 'how', 'buying', 'food', 'problems', 'garbage', 'quit', 'mr', 'so', 'cute', 'stupid', 'stop', 'dog', 'love', 'my']\n",
      "[0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 测试，先加载数据，在创建单词列表，\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "Vec0 = setOfWords2Vec(myVocabList, listOPosts[0])\n",
    "Vec3 = setOfWords2Vec(myVocabList, listOPosts[3])\n",
    "print(myVocabList)\n",
    "print(Vec0)\n",
    "print(Vec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train: calculating probabilities from word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c_i|w)=\\frac{p(w|c_i)p(c_i)}{p(w)}$$  \n",
    "\n",
    "- $p(w)$ is sum of the probability of each word occurrence in the list. This item could be omitted since both P(0) and P(1) include it.\n",
    "- $p(w|c_i)$ is the production of each word occurrence in class 1 or class 2.  \n",
    "- $p(c_i)$ is the probability of the class 1 or class 2 in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.675586Z",
     "start_time": "2018-12-24T14:14:16.656585Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# trainMatrix 为文档矩阵，和每篇文档所对应的标签\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    # 获得文档矩阵的长度\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 得到输入矩阵中列的长度\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # caculate the probability of abusive sentence\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    '''\n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    '''\n",
    "    # to lessen the impact of 0 in mutiplication,the code above need to be changed as below\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # the probability for each word which comes from p1 or p0\n",
    "    '''\n",
    "    ln(a*b)  =  ln(a)+ln(b)\n",
    "    p1Vect = p1Num / p1Denom\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    '''\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.694587Z",
     "start_time": "2018-12-24T14:14:16.679586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0V: [-2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.15948425\n",
      " -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.56494936 -3.25809654 -3.25809654\n",
      " -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -1.87180218]\n",
      "p1V: [-3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -1.94591015\n",
      " -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -2.35137526\n",
      " -3.04452244 -3.04452244 -3.04452244 -1.65822808 -2.35137526 -1.94591015\n",
      " -3.04452244 -3.04452244]\n",
      "pAb: 0.5\n"
     ]
    }
   ],
   "source": [
    "#加载测试数据，返回6个sentences以及6句话对应的标签。\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "#根据以上生成的6个sentences中的单词，创建一个不包含重复吃的集合。\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "#矩阵，将每句话中的单词映射到集合中。1表示存在。\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "print('p0V:', p0V)\n",
    "print('p1V:', p1V)\n",
    "print('pAb:', pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: modifying the classifier for real-world conditions \n",
    "- Naïve Bayes classify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.711588Z",
     "start_time": "2018-12-24T14:14:16.698587Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.727589Z",
     "start_time": "2018-12-24T14:14:16.715588Z"
    }
   },
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPost, listClasses = loadDataSet()\n",
    "    myVocaList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.746590Z",
     "start_time": "2018-12-24T14:14:16.737589Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as :  0\n",
      "['stupid', 'garbage'] classified as :  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare: the bag-of-words document model\n",
    "- Naïve Bayes bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.763591Z",
     "start_time": "2018-12-24T14:14:16.751590Z"
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: classifying spam email with naïve Bayes\n",
    "### Prepare: tokenizing text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.778592Z",
     "start_time": "2018-12-24T14:14:16.766591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "print(mySent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.791592Z",
     "start_time": "2018-12-24T14:14:16.781592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "'''\n",
    "r = re.compile('\\\\W*')\n",
    "listOfTokens = r.split(mySent)\n",
    "'''\n",
    "r = re.compile('\\\\w+')\n",
    "listOfTokens = r.findall(mySent)\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.805593Z",
     "start_time": "2018-12-24T14:14:16.793593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "allWords = [tok for tok in listOfTokens if len(tok) > 0]\n",
    "print(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T15:05:06.991054Z",
     "start_time": "2018-12-24T15:05:06.976053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'Google', 'Groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'February', '2011', 'We', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'Google', 'Groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'Instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'Google', 'Docs', 'and', 'Google', 'Sites', 'For', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'Google', 'Sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'You', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'If', 'you抮e', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'Google', 'Docs', 'You', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'Google', 'Groups']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "emailText = open('email/ham/6.txt').read()\n",
    "'''\n",
    "r = re.compile('\\\\W*')\n",
    "listOfTokens = r.split(emailText)\n",
    "'''\n",
    "r = re.compile('\\\\w+')\n",
    "listOfTokens = r.findall(emailText)\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: cross validation with naïve Bayes \n",
    "-  File parsing and full spam test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.849596Z",
     "start_time": "2018-12-24T14:14:16.840595Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove the punctuations and return the words above 2 characters long only\n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\w*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:16.871597Z",
     "start_time": "2018-12-24T14:14:16.853596Z"
    }
   },
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(1, 26):  # total 26 text files\n",
    "        wordList = textParse(\n",
    "            open('email/spam/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(\n",
    "            open('email/ham/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)  # append as one element\n",
    "        fullText.extend(wordList)  # to extend original list\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)  # make the words unique\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []\n",
    "    for i in range(10):  # extract the testing matrix randomly\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del (trainingSet[randIndex])\n",
    "    # the sub-matrix for training\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V,\n",
    "                      pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    #print(\"the error rate is: \", float(errorCount) / len(testSet))\n",
    "    return float(errorCount) / len(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:17.938658Z",
     "start_time": "2018-12-24T14:14:16.875597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0929999999999999\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(100):\n",
    "    result.append(spamTest())\n",
    "print(sum(result) / len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T03:37:22.227469Z",
     "start_time": "2018-12-24T03:37:22.182466Z"
    }
   },
   "source": [
    "------------------------------------------------------------\n",
    "## Example: using naïve Bayes to reveal local attitudes from personal ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  To test following code, the feedparser need to be installed http://code.google.com/p/feedparser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:25.258077Z",
     "start_time": "2018-12-24T14:14:17.944658Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse('http://feeds.bbci.co.uk/news/rss.xml')\n",
    "print(len(ny['entries']))\n",
    "sf = feedparser.parse('http://rss.cnn.com/rss/edition.rss')\n",
    "print(len(sf['entries']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RSS feed classifier and frequent word removal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:25.271077Z",
     "start_time": "2018-12-24T14:14:25.261077Z"
    }
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList, fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)\n",
    "    sortedFreq = sorted(\n",
    "        freqDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedFreq[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:25.298079Z",
     "start_time": "2018-12-24T14:14:25.274078Z"
    }
   },
   "outputs": [],
   "source": [
    "def localWords(feed1, feed0):\n",
    "    import feedparser\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    minLen = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    top30Words = calcMostFreq(vocabList, fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = list(range(2 * minLen))\n",
    "    testSet = []\n",
    "    for i in range(20):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del (trainingSet[randIndex])\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector),p0V,p1V,pSpam) != \\\n",
    "            classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is: ', float(errorCount) / len(testSet))\n",
    "    return vocabList, p0V, p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:25.864111Z",
     "start_time": "2018-12-24T14:14:25.302079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.55\n",
      "the error rate is:  0.6\n"
     ]
    }
   ],
   "source": [
    "ny = feedparser.parse('http://feeds.bbci.co.uk/news/world/rss.xml')\n",
    "sf = feedparser.parse('http://rss.cnn.com/rss/edition_world.rss')\n",
    "vocabList, pSF, pNY = localWords(ny, sf)\n",
    "vocabList, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze: displaying locally used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:25.880112Z",
     "start_time": "2018-12-24T14:14:25.868112Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V = localWords(ny, sf)\n",
    "    topNY = []\n",
    "    topSF = []\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0: topSF.append((vocabList[i], p0V[i]))\n",
    "        if p1V[i] > -6.0: topNY.append((vocabList[i], p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print(item[0])\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY **\")\n",
    "    for item in sortedNY:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T14:14:25.903114Z",
     "start_time": "2018-12-24T14:14:25.883112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.5\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY **\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
