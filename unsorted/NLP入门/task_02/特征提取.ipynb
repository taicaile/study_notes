{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大匹配是指以词典为依据，取词典中最长单词为第一个次取字数量的扫描串，在词典中进行扫描（为提升扫描效率，还可以跟据字数多少设计多个字典，然后根据字数分别从不同字典中进行扫描）。例如：词典中最长词为“中华人民共和国”共7个汉字，则最大匹配起始字数为7个汉字。然后逐字递减，在对应的词典中进行查找。\n",
    "\n",
    "下面以“我们在野生动物园玩”详细说明一下这几种匹配方法： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1、正向最大匹配法： \n",
    "```\n",
    "    正向即从前往后取词，从7->1，每次减一个字，直到词典命中或剩下1个单字。 \n",
    "    第1次：“我们在野生动物”，扫描7字词典，无 \n",
    "    第2次：“我们在野生动”，扫描6字词典，无 \n",
    "    。。。。 \n",
    "    第6次：“我们”，扫描2字词典，有 \n",
    "    扫描中止，输出第1个词为“我们”，去除第1个词后开始第2轮扫描，即： \n",
    "    第2轮扫描： \n",
    "    第1次：“在野生动物园玩”，扫描7字词典，无 \n",
    "    第2次：“在野生动物园”，扫描6字词典，无 \n",
    "    。。。。 \n",
    "    第6次：“在野”，扫描2字词典，有 \n",
    "    扫描中止，输出第2个词为“在野”，去除第2个词后开始第3轮扫描，即： \n",
    "    第3轮扫描： \n",
    "    第1次：“生动物园玩”，扫描5字词典，无 \n",
    "    第2次：“生动物园”，扫描4字词典，无 \n",
    "    第3次：“生动物”，扫描3字词典，无 \n",
    "    第4次：“生动”，扫描2字词典，有 \n",
    "    扫描中止，输出第3个词为“生动”，第4轮扫描，即： \n",
    "    第4轮扫描： \n",
    "    第1次：“物园玩”，扫描3字词典，无 \n",
    "    第2次：“物园”，扫描2字词典，无 \n",
    "    第3次：“物”，扫描1字词典，无 \n",
    "    扫描中止，输出第4个词为“物”，非字典词数加1，开始第5轮扫描，即： \n",
    "    第5轮扫描： \n",
    "    第1次：“园玩”，扫描2字词典，无 \n",
    "    第2次：“园”，扫描1字词典，有 \n",
    "    扫描中止，输出第5个词为“园”，单字字典词数加1，开始第6轮扫描，即： \n",
    "    第6轮扫描： \n",
    "    第1次：“玩”，扫描1字字典词，有 \n",
    "    扫描中止，输出第6个词为“玩”，单字字典词数加1，整体扫描结束。 \n",
    "    正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，单字字典词为2，非词典词为1。 \n",
    "```\n",
    "- 2、逆向最大匹配法：\n",
    "```\n",
    "    逆向即从后往前取词，其他逻辑和正向相同。即： \n",
    "    第1轮扫描：“在野生动物园玩” \n",
    "    第1次：“在野生动物园玩”，扫描7字词典，无 \n",
    "    第2次：“野生动物园玩”，扫描6字词典，无 \n",
    "    。。。。 \n",
    "    第7次：“玩”，扫描1字词典，有 \n",
    "    扫描中止，输出“玩”，单字字典词加1，开始第2轮扫描 \n",
    "    第2轮扫描：“们在野生动物园” \n",
    "    第1次：“们在野生动物园”，扫描7字词典，无 \n",
    "    第2次：“在野生动物园”，扫描6字词典，无 \n",
    "    第3次：“野生动物园”，扫描5字词典，有 \n",
    "    扫描中止，输出“野生动物园”，开始第3轮扫描 \n",
    "    第3轮扫描：“我们在” \n",
    "    第1次：“我们在”，扫描3字词典，无 \n",
    "    第2次：“们在”，扫描2字词典，无 \n",
    "    第3次：“在”，扫描1字词典，有 \n",
    "    扫描中止，输出“在”，单字字典词加1，开始第4轮扫描 \n",
    "    第4轮扫描：“我们” \n",
    "    第1次：“我们”，扫描2字词典，有 \n",
    "    扫描中止，输出“我们”，整体扫描结束。 \n",
    "    逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，单字字典词为2，非词典词为0。 \n",
    "```\n",
    "- 3、双向最大匹配法： \n",
    "```\n",
    "    正向最大匹配法和逆向最大匹配法，都有其局限性，我举得例子是正向最大匹配法局限性的例子，逆向也同样存在（如：长春药店，逆向切分为“长/春药店”），因此有人又提出了双向最大匹配法，双向最大匹配法。即，两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。 \n",
    "    如：“我们在野生动物园玩” \n",
    "    正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，两字词3个，单字字典词为2，非词典词为1。 \n",
    "    逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，五字词1个，两字词1个，单字字典词为2，非词典词为0。 \n",
    "    非字典词：正向(1)>逆向(0)（越少越好） \n",
    "    单字字典词：正向(2)=逆向(2)（越少越好） \n",
    "    总词数：正向(6)>逆向(4)（越少越好） \n",
    "    因此最终输出为逆向结果。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 2), ('python', 2), ('I', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 词，字符频率统计\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "file_path = ''\n",
    "\n",
    "# words = re.findall(r'\\w+', open(file_path).read().lower())\n",
    "words = re.findall(r'\\w+', 'I like python, like python, ')\n",
    "print(Counter(words).most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型中unigram、bigram、trigram的概念\n",
    "- 如果一个词的出现与它周围的词是独立的，那么我们就称之为unigram也就是一元语言模型\n",
    "- 如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram\n",
    "- 假设一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        # n is the order of n-gram language model\n",
    "        self.n = n\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "\n",
    "    # scan a sentence, extract the ngram and update their\n",
    "    # frequence.\n",
    "    #\n",
    "    # @param    sentence    list{str}\n",
    "    # @return   none\n",
    "    def scan(self, sentence):\n",
    "        # file your code here\n",
    "        for line in sentence:\n",
    "            self.ngram(line.split())\n",
    "        #unigram\n",
    "        if self.n == 1:\n",
    "            try:\n",
    "                fip = open(\"data.uni\",\"w\")\n",
    "            except:\n",
    "                print >> sys.stderr ,\"failed to open data.uni\"\n",
    "            for i in self.unigram:\n",
    "                fip.write(\"%s %d\\n\" % (i,self.unigram[i]))\n",
    "        if self.n == 2:\n",
    "            try:\n",
    "                fip = open(\"data.bi\",\"w\")\n",
    "            except:\n",
    "                print >> sys.stderr ,\"failed to open data.bi\"\n",
    "            for i in self.bigram:\n",
    "                fip.write(\"%s %d\\n\" % (i,self.bigram[i]))\n",
    "    # caluclate the ngram of the words\n",
    "    #\n",
    "    # @param    words       list{str}\n",
    "    # @return   none\n",
    "    def ngram(self, words):\n",
    "        # unigram\n",
    "        if self.n == 1:\n",
    "            for word in words:\n",
    "                if word not in self.unigram:\n",
    "                    self.unigram[word] = 1\n",
    "                else:\n",
    "                    self.unigram[word] = self.unigram[word] + 1\n",
    "\n",
    "        # bigram\n",
    "        if self.n == 2:\n",
    "            num = 0\n",
    "            stri = ''\n",
    "            for i in words:\n",
    "                num = num + 1\n",
    "                if num == 2:\n",
    "                    stri  = stri + \" \"\n",
    "                stri = stri + i\n",
    "                if num == 2:\n",
    "                    if stri not in self.bigram:\n",
    "                        self.bigram[stri] = 1\n",
    "                    else:\n",
    "                        self.bigram[stri] = self.bigram[stri] + 1\n",
    "                    num = 0\n",
    "                    stri = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 文本矩阵化：要求采用词袋模型且是词级别的矩阵化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理的一些任务中，我们总是要先将源数据处理成符合模型算法输入的形式，在经过分词、停用词移除等其他操作之后，我们一般需要将文本转化成矩阵的形式，这一步也被称为特征提取（feature extraction）或者向量化（vectorization）。常见的有, 词袋模式(BoW)，tf-idf，Hash，lsa, lda, Doc2vec这几种方法的概念和使用方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
